# LocalRAG-Agent
# Local AI Agent using Ollama, LangChain, ChromaDB & RAG

This project implements a **Local AI agent** that runs entirely on a personal machine using open-source tools.
The agent uses a **Retrieval-Augmented Generation (RAG)** pipeline to answer user queries based on locally stored documents.
No cloud-based LLM APIs are used, ensuring **data privacy**, **offline capability**, and **full local control**.

## Project Overview

The system allows users to query a set of local documents and receive context-aware responses generated by a locally hosted LLM.
Relevant document chunks are retrieved using vector similarity search and injected into the prompt before response generation.
This project focuses on **system design clarity** rather than UI or large-scale optimization.

## Key Features

- Fully local LLM inference using **Ollama**
- Retrieval-Augmented Generation (RAG) pipeline
- Vector storage and similarity search using **ChromaDB**
- Agent orchestration using **LangChain**
- Runs offline with no external API dependency
- Modular and extensible Python codebase

## Tech Stack

- **Language:** Python  
- **LLM Runtime:** Ollama  
- **Agent Framework:** LangChain  
- **Vector Database:** ChromaDB  
- **Embedding Model:** Lmxbai-embed-large (via Ollama) 

## System Architecture

User Query
↓
LangChain Agent
↓
RAG Pipeline
↓
ChromaDB (Vector Similarity Search)
↓
Relevant Document Chunks
↓
Prompt Augmentation
↓
Local LLM (Ollama)
↓
Final Response

## How RAG is Implemented

1. Local documents are loaded from a predefined directory.
2. Documents are chunked into smaller text segments.
3. Each chunk is converted into vector embeddings using a local embedding model.
4. Embeddings are stored in ChromaDB.
5. At query time:
   - The user query is embedded
   - Similar document chunks are retrieved from ChromaDB
   - Retrieved context is injected into the LLM prompt
6. The local LLM generates a grounded response based on the retrieved context.

## Project Structure

local-ai-agent-rag/
│
├── data/ # Source documents
├── vectordb/ # ChromaDB persistent storage
├── main.py # Application entry point
├── requirements.txt # Python dependencies
└── README.md



